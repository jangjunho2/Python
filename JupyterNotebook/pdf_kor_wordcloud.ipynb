{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그냥 학교에서 교양으로 txt가져와서 단어구름 했던거 가지고 pdf 여러장으로도 만들어본거..\n",
    "친구 단어구름 필요하다해서 해본거 최적화 X 돌아는감..\n",
    "\n",
    "wordcloud 설치 오류 날떄 참고:\n",
    "https://hcid-courses.github.io/TA/FAQ/python_wordcloud_troubleshoot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rldir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# pip install Pillow numpy pandas PyPDF2 nltk wordcloud konlpy google-colab\n",
    "# pip install wordcloud --use-pep517\n",
    "from konlpy.tag import Hannanum\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import konlpy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "hannanum = Hannanum() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf 갯수 입력\n",
    "nums_pdf = 1  # @개\n",
    "lines = \"\"\n",
    "\n",
    "for i in range(nums_pdf):\n",
    "    # 한국어 전용 파일이름 krJam0.pdf , krJam1.pdf 무조건 0부터 해야함\n",
    "    # pdf_file = open(f'/content/drive/My Drive/wordcloud/krJam{i}.pdf', 'rb') #경로 맞게 수정\n",
    "    # 한국어 전용 파일이름 krjam0.pdf , kerJam1\n",
    "    # pdf_file = open(f'/content/drive/My Drive/krJam{i}.pdf', 'rb')\n",
    "    pdf_file = open(f'C:/Users/rldir/OneDrive/바탕 화면/jams/krJam{i}.pdf', 'rb')\n",
    "\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    for page in pdf_reader.pages:\n",
    "        lines += page.extract_text()\n",
    "\n",
    "    pdf_file.close()\n",
    "\n",
    "\n",
    "lines_list = lines.splitlines()\n",
    "words_list = [lines.split() for line in lines_list]\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    flatList = []\n",
    "    for elem in l:\n",
    "        if type(elem) == list:\n",
    "            for e in elem:\n",
    "                flatList.append(e)\n",
    "        else:\n",
    "            flatList.append(elem)\n",
    "    return flatList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6835590\n"
     ]
    }
   ],
   "source": [
    "double_list = words_list\n",
    "single_list = [item for sublist in double_list for item in sublist]\n",
    "lines = single_list\n",
    "print(len(lines))  # 저번에 한개 햇을떄 683만이었나 그거보다 크면 될듯?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 8개의 코어가 감지되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"총 {num_cores}개의 코어가 감지되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존코드\n",
    "\n",
    "############################# 오래걸림##########주의########\n",
    "temp = [] \n",
    "for i in range(len(lines)):\n",
    "    print(lines[i]) #줠라김... #되는지 테스트 중단하고 ctrl+/ 하고 다시 실행 ㄱㄱ\n",
    "    temp.append(hannanum.nouns(lines[i])) #명사만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt로 멀티프로세싱 한 코드\n",
    "# import multiprocessing as mp\n",
    "\n",
    "# def get_nouns(line):\n",
    "#     return hannanum.nouns(line)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     temp = [] \n",
    "#     pool = mp.Pool(processes=3) # 코어 개수에 맞게 조정\n",
    "#     for i in range(len(lines)):\n",
    "#         print(i)\n",
    "#         #print(lines[i]) #줠라김... #되는지 테스트 중단하고 ctrl+/ 하고 다시 실행 ㄱㄱ\n",
    "#         temp.append(pool.apply_async(get_nouns, (lines[i],)).get()) #명사만 저장\n",
    "\n",
    "#     pool.close()\n",
    "#     pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = flatten(temp)\n",
    "word_list = pd.Series([x for x in word_list if len(x) > 1])\n",
    "word_list.value_counts().head(30)  # wordcloud에 넣을 빈도 높은 숫자 @개 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path = '/content/drive/My Drive/NanumBarunGothic.ttf'\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    font_path=font_path,\n",
    "    width=800,  # 가로크기 #저장하는거에 영향 미침ㅇㅇ\n",
    "    height=800,  # 세로크기\n",
    "    background_color=\"white\"  # 뒷 배경\n",
    ")\n",
    "\n",
    "count = Counter(word_list)\n",
    "wordcloud = wordcloud.generate_from_frequencies(count)\n",
    "\n",
    "\n",
    "def __array__(self):\n",
    "    \"\"\"Convert to numpy array.\n",
    "    Returns\n",
    "    -------\n",
    "    image : nd-array size (width, height, 3)\n",
    "        Word cloud image as numpy matrix.\n",
    "    \"\"\"\n",
    "    return self.to_array()\n",
    "\n",
    "\n",
    "def to_array(self):\n",
    "    \"\"\"Convert to numpy array.\n",
    "    Returns\n",
    "    -------\n",
    "    image : nd-array size (width, height, 3)\n",
    "        Word cloud image as numpy matrix.\n",
    "    \"\"\"\n",
    "    return np.array(self.to_image())\n",
    "\n",
    "\n",
    "array = wordcloud.to_array()\n",
    "print(count)  # 클라우드에 들어갈 것들 이상한거 보고 제거하기 (출력 윗 줄)\n",
    "# 밑에 불필요한거 입력\n",
    "del_items = \"\"  # 불필요한거 입력 ㄱㄱ\n",
    "\n",
    "del_items = del_items.split()\n",
    "\n",
    "for del_item in del_items:\n",
    "    del count[del_item]\n",
    "###############################\n",
    "print(count)  # 제거된 이후 결과 실행하면 나오는 줄중에 (밑에 줄)\n",
    "\n",
    "wordcloud = wordcloud.generate_from_frequencies(count)\n",
    "array = wordcloud.to_array()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':  # 대체 코드\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 얼마나 크게 보여주냐 #저장하는거에 영향 X 밑에 보여주기 크기용 #아마도?\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(array, interpolation=\"bilinear\")\n",
    "plt.show()  # 보여주기\n",
    "fig.savefig('wordcloud.png')  # 파일이 저장되는 이름 ㅇㅇ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
